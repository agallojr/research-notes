
-- So it begins...  --

After some months, perhaps more than a year, we've been tracking the quantum computing space with increasing interest.  Prior to that it was the stuff of science fiction, more recently more akin to cold fusion - we'll believe it when we see it, maybe someday in the far distant future it will be useful for something.

Quantum is clearly still in its infancy.  I had dinner with a PhD physicist at SC23 and he admitted the best medium-term application of quantum computing is to model quantum systems.  Because otherwise, if there are applications of interest besides cryptography, we don't know about them as most people have no idea how to program these things.  

And on first blush, they are entirely foreign.  Not like OO was to procedural.  A real mind f***.  We can sit in 10 hours of online tutorials on quantum and still never write "hello world", add two numbers.  

To compound matters, the base physics which is used to build these non-von Neumann machines varies.  For example, Xanadu.ai funds and supports two open source libraries, depending on the underlying physics of the machine.  Some are gate-based, some are not.

Walled gardens are already springing up in this greenfield.  Most notably NVIDIA.  While cuQuantum promises to bring performant quantum simulation to enterprise NVIDIA customers, those unable to buy a DGX are shut out (or sent to the cloud).  And while NVIDIA promotes heterogeneous architectures (e.g. Grace Hopper), they seem primarily concerned with heterogeneity in the context of their own product line.  Heterogeneous systems will likely be the norm in the quantum space - CPUs, GPUs, QPUs, and so on.  Within these classes of chips, we have chiplets.  Clearly the enterprise must be concerned with managing heterogeneity in their codebase which often contains critical IP.  

The choice of language itself must be made, again, as it was with HPC and GPUs.  Here Python does seem to have an early lead over C++, but there are lower assembly standards, and given the variation in the hardware physics, standards which can still take 100% most performant use of the hardware will likely take a while to emerge, and until then arguments with those who put performance first will be necessary.

Enterprises should prepare for an unmaintainable debris field from their scientific and engineering programmers in both the LLM and quantum spaces, and given its more maturity and alignment with current software practices, the debris field in LLM extends and blooms for business apps too (apps, and models, including personal ones... an operational nightmare for teams not ready to deal).

As in GPUs with CUDA and more portable abstractions like SYCL, IBM, ironically to the senior staff, leads the way in support of open standards.  Qiskit is the top platform for quantum today in terms of installed base, and it promises to support hardware other than just IBM's.  We also see frameworks putting Qiskit under the hood, supporting a "driver for Qiskit".

Qiskit quantum simulations can run on commodity hardware up to a small number of qubits, but enough to get started.  We're also interested in PennyLane supported by Xanadu, but it does more than just support programming gates, and we're not prepared to take on that.  

Hardware, and support in simulators, will limit what can be done.  For example, not only are total qubits limited to maybe a dozen or two, in many cases 3-qubit gates are not yet supported by the simulators.

So for the moment we'll play with Qiskit and see what happens, then perhaps try the similar-to with PennyLane and gates, and then get into the stuffier stuff, and some of the potential applications.


-- Qiskit -- 

Quantum seems to be in a similar situation as the LLMs... the documentation stinks, examples don't work, etc.  After a few googles, RTFMs, and various failed attempted, I did get a basic null circuit working on the IBM simulator.  A login is required on the IBM site, even if the local simulator is used (not too cool).  In theory every new user gets some IBM cloud credits, so we should be able to run the example up there too, but we'll wait for now.  

On run of a working circuit, we see this:

(qiskit) {develop} ~/src/research-notes/src/qiskit $ python hello_world_1.py 
/home/agallojr/src/research-notes/src/qiskit/hello_world_1.py:14: DeprecationWarning: Circuits that do not match the target hardware definition will no longer be supported after March 1, 2024. See the transpilation documentation (https://docs.quantum.ibm.com/transpile) for instructions to transform circuits and the primitive examples (https://docs.quantum.ibm.com/run/primitives-examples) to see this coupled with operator transformations.
  job = Sampler(backend).run(qc)
job id: cnbps1nmo951g805let0
SamplerResult(quasi_dists=[{0: 1.0}], metadata=[{'shots': 4000, 'circuit_metadata': {}}])

That's like a week from now.  The complaint seems to be with this:

    job = Sampler(backend).run(qc)

We'll look into the evolving signatures and try to stay with the tip.


-- Alternative simulators, alternate SDKs - PennyLane -- 

Aer is a simulator, similar to the IBM QASM.  The latter requires an IBM login, the former does not (though it seems to create a null IBM config file anyway? cuz Qiskit?).  Aer package itself contains more than one simulator, for different noise profiles, etc.  We'll have to look into the differences at some point.

Transpiling can be used to target the circuit on a specific simulator, pre-chew it for performance.  Not surprising, GPU support on Windows Linux (WSL) is not supported at least with my commodity GPU.

PennyLane is a competitor to Qiskit, and supports a plugin for Qiskit, thus permitting one to use PennyLane vs. IBM simulators and hardware.  Config files are used to specify the hardware device.  Qiskit code can be imported wholely and used with PennyLane-managed devices and gradient algorithms.  Compilation of circuits for specific platforms with Catalyst is also possible.

The Canadian government has just given parent firm Xanadu a cash influx to aid in dissemination and adoption of the technology.  This is based in southern Ontario.  (D-Wave is also based in Canada.) Upfront Xanadu claims the goal is to democratize.  First contact with their installation instructions was a good experience, examples worked relatively painlessly.

We've now written a basic circuit and using PennyLane we've run it on a variety of simulators and real machines at AWS and IBM.  And it was remarkably easy.  Tho we notice that in both cloud cases the local Python process idled while the remote process was in queue and then running.  Clearly there needs to be / is some kind of batch submission idiom to investigate.  Its also remarkable (of course) that the cloud providers match our lwfm model of sites and compute types.

Also, while we'd like to run the same circuit on NVIDIA hardware, there's a high bar (https://nvidia.github.io/cuda-quantum/latest/install.html#dependencies-and-compatibility).  If the hardware does not support the necessary version of CUDA, the simulator will revert to the non-GPU version.  There is a cloud for NVIDIA, but there's a salesman contact sheet... no go for me.  AWS and IBM were self-service sign-ups. 

Also clearly we need to know more about leveraging a quantum step in a broader "hybrid" computation or workflow.  It would seem that type 1 workflows are the only real area of interest, as type 2 and 3 involve distances and delay not within normal "HPC" expectation, and are thus no different than any other app running on any compute site.

We'll look at PennyLane and MPI support.


-- Problem morphing into quantum forms --

Taking a problem and morphing it into a circuit is a problem.  It reminds of proving a problem is NP Complete.  (Insert horror story from grad school algorithms class here.)  In theory, given LLM planning abilities, an LLM should be helpful in problem transformation.  Also, in gate based quantum models, the set of verbs is small, thus the potential for a coding assistant which implements the transformed problem seems plausible.  We will look into that.

But circuits are not the only computing paradigm (pulse photon, quantum neuromorphic, adiabatic / annealing, etc.) and the algorithms are sensitive to or even dependent upon the underlying physics.  e.g. a brain-inspired universal quantum perceptron is emergent, as yet to be agreeably specified.  QASM is a good place to start, but will also likely come under pressure from the variation in emerging hardware.  Much work is being done with the researcher only running on a simulator, as the real machine isn't yet realized, simply to better understand the range of the possible (e.g. divide and conquer as applied to quantum algorithms).  There's even some back-and-forth such as using quantum computing to understand quantum state physics, and using quantum state physics to understand quantum computing.


-- GPU scarcity -- 

Its real.  AWS is stingy on giving out quota.  Took three rounds with AWS tech support and then I got 1 node when I asked for 3.  While waiting, I tried getting a node from Google, and they had none in numerous US and CA data centers I tried.  Bitcoin must be up...  

Spent several days trying to get the PennyLane GPU install with CUDA and MPI working on an AWS machine with a V100 GPU.  Only the NVIDIA PennyLane simulator incorporates MPI.  And I come to find out from the PennyLane user group that the cuQuantum implementation on which it depends has a 2 GPU per node minimum.  This would mean going back to AWS and asking for more quota.  

The home company has machines with at least one GPU of the proper kind.  I'm not sure it has nodes with two GPUs - it can be an inquiry down the road.  

Suffice it to say that there exists an MPI-aware simulator for PennyLane (and for cuQuantum for that matter) but it has a high bar for installation.  We will defer this inquiry for now.

The interchange with the PennyLane tech support is here:

        Hey <@698521488508321895> , glad to hear you are enjoying the software. The MPI backend for lightning.gpu is mainly built for HPC-system installs (we use it regularly on Perlmutter, amongst other systems), and so it has some quirks to ensure the MPI env is correctly supported. We do have CI testing using both MPICH and OpenMPI , which if the instructions aren't working for you, the steps listed here may help reproducing a working env ( see here and below for the Python test-env setup https://github.com/PennyLaneAI/pennylane-lightning/blob/bb9b1443efd6e3f4e6be0865303f92b4a1c733f9/.github/workflows/tests_linux_x86_mpi_gpu.yml#L238). In this case, we are also limited by the MPI support offered by cuQuantum, namely it requires at least 2 GPUs per node, and only a power-of-2 for global MPI ranks.

        As to why we do not have MPI on other devices, the main reason for us is prioritization --- we focus on adding features that are on our roadmap. As of right now, we do not have MPI support for lightning.qubit or lightning.kokkos in the coming months, but that isn't to say it won't be prioritized at some point, given enough request or needs.

        If you are interested, you can build such an env using a HPC-system provided toolchain like follows (these instructions work on Perlmutter for example):
        https://discuss.pennylane.ai/t/pennylane-multi-gpu-script-fails-with-error-even-there-are-enough-gpus/3978/10?u=mlxd


-- NVIDIA AI Woodstock -- 

Followed by the AI Altamont... please people.

NVIDIA claims interoperability with all the major quantum hardware makers, a heterogeneous ecosystem, cloud hosting, etc.

I finally did get sufficient quota from AWS to get a host with GPUs needed for the PennyLane GPU install with MPI with depends on CUDA, but I struggled as usual with the evil dependencies between CUDA and MPI.  I tried installing a GPU accelerated version of cuQuantum on the laptop, but that had similar issues.  I was able to get a non-GPU accelerated version of cuQuantum running on the laptop, but big deal - I'd rather use PennyLane and their CUDA device driver.

Perhaps a Docker image will be useful.  I did try and use the one provided by NVIDIA for AWS - its 11 months old, and is built on an older version, and one not supported by the newest quantum tools.  Trying to install an update into the container was a fail.

Note: if you go to the NVIDIA linked location for their presumably supported Docker container outside of AWS, it too is the older version.  In other words, the documentation is a joke, the tools are old, the installation is obnoxious, and its a walled garden.  (see: https://catalog.ngc.nvidia.com/orgs/nvidia/containers/cuda-quantum)

Our research team will of course align with the IT team and provide servers, so I fully expect at the end of the day NVIDIA will be used by the business researchers, but I will continue to use PennyLane for my own learning.  Its portable, it does the differentiation (which I still don't really understand), and it does pulses for optical systems.  It plenty good enough.










